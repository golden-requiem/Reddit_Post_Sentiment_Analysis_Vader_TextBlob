{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ba152e-0f88-4156-85aa-88d95ba492e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "REDDIT POST DEPRESSION SENTIMENT ANALYSIS\n",
    "\n",
    "* TFIDF (information extraction)\n",
    "* VADER\n",
    "* TextBlob\n",
    "* HuggingFace\n",
    "\n",
    "import numpy as np #linear\n",
    "import pandas as pd #data processing\n",
    "from sklearn import preprocessing\n",
    "import heapq #heap based priority queue implementattion\n",
    "from collections import Counter #special contained datatypes, \n",
    "# Counter: Dictionary subclass for counting hashable objects. \n",
    "#It provides a convenient way to count the occurrences of elements in a collection.\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords #access to large bodies of text for classification, etc\n",
    "from nltk.tokenize import word_tokenize #word tokenizer\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer #for converting tokens back into text\n",
    "#importing the dataset\n",
    "import os\n",
    "df = pd.read_csv(\"depression_dataset_reddit_cleaned.csv\")\n",
    "\n",
    "Bag of Words representation of data: unordered collection but keeping freqeuncy of words\n",
    "\n",
    "1) tokeninze (remove stop words before)\n",
    "2) unique words list (2, one for depressed and non-depressed)\n",
    "3) word occurences in unique word list\n",
    "4) vectorize\n",
    "\n",
    "#remove stop words\n",
    "s_words = set(stopwords.words('english')) #gets list of stopwords, set converts into set\n",
    "rows = df.shape[0]\n",
    "#remove stop words in each row\n",
    "cleaned = []\n",
    "for i in range(rows):\n",
    "    tokens = word_tokenize(df.clean_text[i]) #tokenize, and create new column\n",
    "    #remove stopwords\n",
    "    for x in tokens:\n",
    "        if x not in s_words:\n",
    "            cleaned.append(x)\n",
    "    \n",
    "    df.clean_text[i] = TreebankWordDetokenizer().detokenize(cleaned) #detokenizes cleaned sentences into original text and into the new_text column\n",
    "    cleaned.clear()\n",
    "\n",
    "#find word frequency\n",
    "d_freq = {}\n",
    "nd_freq = {}\n",
    "for i in range(rows):\n",
    "    tokens = word_tokenize(df.clean_text[i])\n",
    "    for x in tokens:\n",
    "        if df.is_depression[i] == 1:\n",
    "            d_freq[x] = 1 if x not in d_freq else d_freq[x] + 1\n",
    "        else:\n",
    "            nd_freq[x] = 1 if x not in nd_freq else nd_freq[x] + 1\n",
    "            \n",
    "\n",
    "d_words = heapq.nlargest(100, d_freq, key=d_freq.get) #gets top 100 words  from d_freq based on frequency\n",
    "nd_words = heapq.nlargest(100, nd_freq, key=nd_freq.get)\n",
    "d_most_f = {}\n",
    "nd_most_f = {}\n",
    "#iterate over each queue and assign the frequency from the queue to the dictionary\n",
    "for j in d_words:\n",
    "    d_most_f[j] = d_freq[j]\n",
    "for j in nd_words:\n",
    "    nd_most_f[j] = nd_freq[j]\n",
    "\n",
    "\n",
    "total_f = Counter(d_most_f) + Counter(nd_most_f)\n",
    "#total_f\n",
    "\n",
    "create arrays for each row in the data based on each word frequency in the data\n",
    "\n",
    "f_vec = []\n",
    "for i in range(rows):\n",
    "    tokens = word_tokenize(df.clean_text[i])\n",
    "    s_vec = []\n",
    "    for x in total_f:\n",
    "        if x in tokens:\n",
    "            #add number of times the word appears in the sentence\n",
    "            count = 0\n",
    "            for j in range(len(tokens)):\n",
    "                if tokens[j] == x:\n",
    "                    count += 1\n",
    "            s_vec.append(count)\n",
    "        else:\n",
    "            s_vec.append(0)\n",
    "    f_vec.append(s_vec)\n",
    "\n",
    "IDF: emphasize words that are rare across the entire corpus but occur frequently in specific documents\n",
    "\n",
    "inv_f = []\n",
    "for x in total_f:\n",
    "    for i in range(rows):\n",
    "        data = 0\n",
    "        tokens = word_tokenize(df.clean_text[i])\n",
    "        if x in tokens:\n",
    "            data += 1\n",
    "    idf = np.log((1+rows)/(1+data)) + 1\n",
    "    inv_f.append(idf)\n",
    "    \n",
    "# idf = ln(1+n/1+df(t)) + 1\n",
    "#IDF(t)=log(N/(df(t) + 1))\n",
    "# n = total number of rows\n",
    "# df(t) # of documents in set that have the term \"t\"\n",
    "\n",
    "print(len(total_f))\n",
    "print(len(f_vec))\n",
    "print(len(inv_f))\n",
    "\n",
    "#ifidf w/ normalization\n",
    "tf = np.array(f_vec)\n",
    "idf_ = np.array(inv_f)\n",
    "tfidf = tf*idf_\n",
    "#normalize data points (0, 1)\n",
    "scale = preprocessing.MinMaxScaler() #scales value to 0 and 1\n",
    "scale.fit(tfidf) #scales each feature to fit between 0 and 1\n",
    "tfidf = scale.transform(tfidf) #each feature in tfidf is scaled to 0 and 1\n",
    "tfidf\n",
    "\n",
    "#print(total_f)\n",
    "#print(tf[0])\n",
    "#print(df.clean_text[0])\n",
    "#print(tfidf[:,1])\n",
    "\n",
    "# iterates over the words in the total_f list and adds columns to the \n",
    "#DataFrame df based on the word frequency in the TF-IDF matrix tfidf.\n",
    "\n",
    "#add columns based on word frequency (total_f)\n",
    "y = 0\n",
    "for x in total_f:\n",
    "    df.loc[:, x] = tfidf[:,y]\n",
    "    y += 1\n",
    "    \n",
    "#df will have additional columns corresponding to each word in the total_f list,\n",
    "#with each column containing the TF-IDF values for that word across all \n",
    "#documents in the dataset. \n",
    "df\n",
    "\n",
    "making new csv file\n",
    "\n",
    "df.to_csv(\"tfdif.csv\", index=False)\n",
    "#most frequent depressed and non depressed words\n",
    "pd.DataFrame(d_words).to_csv(\"100_frequent_depressed.csv\", index=False)\n",
    "pd.DataFrame(nd_words).to_csv(\"100_frequent_n_depressed.csv\", index=False)\n",
    "\n",
    "df_2 = pd.read_csv('tfdif.csv')\n",
    "df_1 = pd.read_csv('depression_dataset_reddit_cleaned.csv')\n",
    "df_1.head()\n",
    "\n",
    "#user vader for sentiment analysis on original data\n",
    "\n",
    "pip install vaderSentiment\n",
    "\n",
    "#lower score = negative sentiment\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentiment = SentimentIntensityAnalyzer()\n",
    "score = sentiment.polarity_scores(df_1.clean_text[50])\n",
    "print(score['compound'])\n",
    "#compound = overall senitment polarity\n",
    "\n",
    "#split training dataset into traain and test datasets\n",
    "\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "\n",
    "#train is the data (posts) to train on (the clean_text, not the altered text because that contains stopwords)\n",
    "\n",
    "train, test = tts(df_1, test_size=0.33, random_state=42)\n",
    "train = train.reset_index(drop=True) #resetting the indexes to make sure there isnt a key error in the following cell\n",
    "train.index\n",
    "\n",
    "#test.drop(columns=['is_depression'], inplace=True)\n",
    "#test.head()\n",
    "\n",
    "#polarity score\n",
    "num_rows = train.shape[0]\n",
    "score_vec = []\n",
    "for i in range(num_rows):\n",
    "    score = sentiment.polarity_scores(train.clean_text[i])\n",
    "    score_vec.append(score['compound'])\n",
    "\n",
    "score_vec[:5]\n",
    "\n",
    "train.loc[:, 'sentiment'] = score_vec\n",
    "train.head()\n",
    "\n",
    "#average, min, and std of sentiment for depressed and non depressed  posts\n",
    "\n",
    "avg = train.groupby(['is_depression'], as_index=False)['sentiment'].mean()\n",
    "print(\"average\")\n",
    "avg\n",
    "\n",
    "standard = train.groupby(['is_depression'], as_index=False)['sentiment'].std()\n",
    "print(\"standard deviation\")\n",
    "standard\n",
    "\n",
    "#average sentiment is lower for 'depressed' posts\n",
    "#the std is greater which mean there is more variance in sentiment in the 'depressed' posts\n",
    "# what could this mean?\n",
    "\n",
    "Using TextBlob\n",
    "\n",
    "!pip install -U textblob\n",
    "!python -m textblob.download_corpora\n",
    "\n",
    "from textblob import TextBlob as tb\n",
    "\n",
    "#split the dataset again\n",
    "\n",
    "tb_train, tb_test = tts(df_1, test_size = 0.33, random_state = 39)\n",
    "tb_train = tb_train.reset_index(drop=True) #resetting the indexes to make sure there isnt a key error in the following cell\n",
    "tb_train.index\n",
    "\n",
    "tb_train.head()\n",
    "\n",
    "sc = tb(tb_train.clean_text[1500])\n",
    "print(sc.sentiment[0])\n",
    "\n",
    "num_rows_2 = tb_train.shape[0]\n",
    "score_vec_2 = []\n",
    "for i in range(num_rows_2):\n",
    "    sc = tb(tb_train.clean_text[i])\n",
    "    score_vec_2.append(sc.sentiment[0])\n",
    "score_vec_2[:5]\n",
    "\n",
    "tb_train.loc[:,'sentiment'] = score_vec_2\n",
    "tb_train\n",
    "\n",
    "average = tb_train.groupby(['is_depression'], as_index=False)['sentiment'].mean()\n",
    "print(\"average\")\n",
    "average\n",
    "\n",
    "stand = tb_train.groupby(['is_depression'], as_index=False)['sentiment'].std()\n",
    "print(\"standard deviation\")\n",
    "stand\n",
    "\n",
    "#TextBlob gives us a higher sentiment for the depressed posts\n",
    "#than the non-depressed posts.\n",
    "#this probably means Vader is a more accurate in this case\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
